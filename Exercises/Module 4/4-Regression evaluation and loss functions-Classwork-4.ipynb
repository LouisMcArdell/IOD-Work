{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression evaluation and loss functions\n",
    "\n",
    "Amin Mehab\n",
    "\n",
    "---\n",
    "\n",
    "### LEARNING OBJECTIVES\n",
    "\n",
    "* After this lesson, you will be able to:\n",
    "    - distinguish different types of loss functions\n",
    "    \n",
    "---\n",
    "\n",
    "### STUDENT PRE-WORK\n",
    "* Before this lesson, you should already be able to\n",
    "    - implement ordinary least squares regression\n",
    "    - evaluate model performance with mean squared error and R squared\n",
    "    - determine the residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions\n",
    "\n",
    "Regressions model the relationship **between predictors and dependent variables**. But the relationship they are measuring and the process of \"fitting\" models is centred around the idea of **loss functions**.\n",
    "\n",
    "The loss function is **what is optimised** by the process of regression. Think of the term \"loss function\" sort of like the **greater the value, the more information about your target variable is \"lost\"** by your model.\n",
    "\n",
    "---\n",
    "\n",
    "## Least squares loss\n",
    "\n",
    "As you may recall from yesterday, the most common loss function in linear regression is the **least squares loss**. It is called least squares loss because it minimizes the sum of the squared errors/residuals.\n",
    "\n",
    "$$\\sum_{i}{\\left(\\hat{y}_i - y_i \\right)^2}$$\n",
    "\n",
    "This is called a **loss function**. The \"loss\" considered is the increasing sum of squared errors, which indicate a **bad fit between predictors and outcome**. We minimize the loss by finding the smallest sum.\n",
    "\n",
    "## Least Absolute Deviations (LDA)\n",
    "\n",
    "An alternative loss function minimizes the **Least Absolute Deviations (LDA)**:\n",
    "\n",
    "$$\\sum_{i}{|\\hat{y}_i - y_i |}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Packages and data\n",
    "\n",
    "Load, for now, the following data and packages.\n",
    "\n",
    "The data is a subset of the football combine dataset. The concept of \"train\" and \"test\" datasets is going to repeatedly come up throughout the course. Imagine training data as the data you have now, and the test data as unobserved data on which you validate the performance of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = pd.read_csv('./datasets/combine_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. Set up variables and build a regression predicting target from predictor (for example, let's predict the height with the weight)\n",
    "\n",
    "Again, your regression should just be a single target and single predictor for now.\n",
    "\n",
    "You can choose any target and predictor that interests you, and you can subset the data if you like as well (subsetting on position, for example, is likely to improve a regression.)\n",
    "\n",
    "You may use statsmodels or scikit-learn to build the regression:\n",
    "\n",
    "```python\n",
    "    import statsmodels.api as sm\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "```\n",
    "\n",
    "Note that sklearn's regression expects your x variable to be a 2D matrix with rows, columns. See here:\n",
    "\n",
    "http://stackoverflow.com/questions/30813044/sklearn-found-arrays-with-inconsistent-numbers-of-samples-when-calling-linearre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Year</th>\n",
       "      <th>Name</th>\n",
       "      <th>Position</th>\n",
       "      <th>HeightFeet</th>\n",
       "      <th>HeightInches</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Arms</th>\n",
       "      <th>Hands</th>\n",
       "      <th>FortyYD</th>\n",
       "      <th>TwentyYD</th>\n",
       "      <th>TenYD</th>\n",
       "      <th>TwentySS</th>\n",
       "      <th>ThreeCone</th>\n",
       "      <th>Vertical</th>\n",
       "      <th>Broad</th>\n",
       "      <th>Bench</th>\n",
       "      <th>Round</th>\n",
       "      <th>College</th>\n",
       "      <th>Pick</th>\n",
       "      <th>PickRound</th>\n",
       "      <th>PickTotal</th>\n",
       "      <th>FirstName</th>\n",
       "      <th>LastName</th>\n",
       "      <th>HeightInchesTotal</th>\n",
       "      <th>Wonderlic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8984</td>\n",
       "      <td>2013</td>\n",
       "      <td>Quanterus Smith</td>\n",
       "      <td>DE</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>250</td>\n",
       "      <td>33.25</td>\n",
       "      <td>10.375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Western Kentucky</td>\n",
       "      <td>13(146)</td>\n",
       "      <td>13</td>\n",
       "      <td>146</td>\n",
       "      <td>Quanterus</td>\n",
       "      <td>Smith</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9002</td>\n",
       "      <td>2013</td>\n",
       "      <td>Abry Jones</td>\n",
       "      <td>DT</td>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>313</td>\n",
       "      <td>35.00</td>\n",
       "      <td>9.750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>26.5</td>\n",
       "      <td>101</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Abry</td>\n",
       "      <td>Jones</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9004</td>\n",
       "      <td>2013</td>\n",
       "      <td>Bennie Logan</td>\n",
       "      <td>DT</td>\n",
       "      <td>6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>309</td>\n",
       "      <td>34.00</td>\n",
       "      <td>10.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.67</td>\n",
       "      <td>7.53</td>\n",
       "      <td>25.0</td>\n",
       "      <td>104</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>LSU</td>\n",
       "      <td>5(67)</td>\n",
       "      <td>5</td>\n",
       "      <td>67</td>\n",
       "      <td>Bennie</td>\n",
       "      <td>Logan</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9012</td>\n",
       "      <td>2013</td>\n",
       "      <td>John Boyett</td>\n",
       "      <td>FS</td>\n",
       "      <td>5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>204</td>\n",
       "      <td>30.50</td>\n",
       "      <td>8.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>24(192)</td>\n",
       "      <td>24</td>\n",
       "      <td>192</td>\n",
       "      <td>John</td>\n",
       "      <td>Boyett</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9018</td>\n",
       "      <td>2013</td>\n",
       "      <td>Bacarri Rambo</td>\n",
       "      <td>FS</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>211</td>\n",
       "      <td>31.00</td>\n",
       "      <td>9.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>23(191)</td>\n",
       "      <td>23</td>\n",
       "      <td>191</td>\n",
       "      <td>Bacarri</td>\n",
       "      <td>Rambo</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  Year             Name Position  HeightFeet  HeightInches  Weight  \\\n",
       "0  8984  2013  Quanterus Smith       DE           6           5.0     250   \n",
       "1  9002  2013       Abry Jones       DT           6           3.0     313   \n",
       "2  9004  2013     Bennie Logan       DT           6           2.0     309   \n",
       "3  9012  2013      John Boyett       FS           5          10.0     204   \n",
       "4  9018  2013    Bacarri Rambo       FS           6           0.0     211   \n",
       "\n",
       "    Arms   Hands  FortyYD  TwentyYD  TenYD  TwentySS  ThreeCone  Vertical  \\\n",
       "0  33.25  10.375      0.0       0.0    0.0      0.00       0.00       0.0   \n",
       "1  35.00   9.750      0.0       0.0    0.0      0.00       0.00      26.5   \n",
       "2  34.00  10.250      0.0       0.0    0.0      4.67       7.53      25.0   \n",
       "3  30.50   8.500      0.0       0.0    0.0      0.00       0.00       0.0   \n",
       "4  31.00   9.250      0.0       0.0    0.0      0.00       0.00       0.0   \n",
       "\n",
       "   Broad  Bench  Round           College     Pick  PickRound  PickTotal  \\\n",
       "0      0      0      5  Western Kentucky  13(146)         13        146   \n",
       "1    101     30      0               NaN      NaN          0          0   \n",
       "2    104     30      3               LSU    5(67)          5         67   \n",
       "3      0     27      6            Oregon  24(192)         24        192   \n",
       "4      0     17      6           Georgia  23(191)         23        191   \n",
       "\n",
       "   FirstName LastName  HeightInchesTotal  Wonderlic  \n",
       "0  Quanterus    Smith               77.0          0  \n",
       "1       Abry    Jones               75.0          0  \n",
       "2     Bennie    Logan               74.0          0  \n",
       "3       John   Boyett               70.0          0  \n",
       "4    Bacarri    Rambo               72.0          0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we subset the data for easier analysis\n",
    "wrs = combine[combine.Position == 'WR']\n",
    "weight = wrs[['Weight']].values\n",
    "height = wrs.HeightInchesTotal.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit your model with your variable like (weight, height):\n",
    "\n",
    "#### With Sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:\n",
    "\n",
    "# mod =\n",
    "\n",
    "# Print coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With StatsModels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:\n",
    "\n",
    "# mod2 =\n",
    "\n",
    "# Print Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Calculate the mean squared error for your regression and the baseline model\n",
    "\n",
    "**Mean squared error** is just the mean of your squared errors. It is typically used as a metric in place of the sum of errors.\n",
    "\n",
    "Either calculate the mean squared error for your regression and baseline model by hand, or use statsmodels/sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this might help you defining the baseline model:\n",
    "# np.tile(np.mean(height), len(height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr_height_hat = mod.predict(weight)\n",
    "\n",
    "mod_mse = mean_squared_error(height, wr_height_hat)\n",
    "base_mse = mean_squared_error(height, np.tile(np.mean(height), len(height)))\n",
    "\n",
    "print ('Model mse:', mod_mse)\n",
    "print ('Baseline mse:', base_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (mod_mse, base_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. Plot \n",
    "### a) the target versus the predictor \n",
    "\n",
    "Add the regression line and the baseline model line for the target versus predictor chart.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression_wbaseline(x, y, yhat):\n",
    "    \n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    ax = fig.gca()\n",
    "    \n",
    "    ymean = np.mean(y)\n",
    "    \n",
    "    ax.scatter(x, y, color='steelblue', s=70, label='true y')\n",
    "    \n",
    "    min_x, max_x = np.min(x), np.max(x)\n",
    "    min_yhat = np.min(yhat)\n",
    "    max_yhat = np.max(yhat)\n",
    "    \n",
    "    ax.plot([min_x, max_x], [min_yhat, max_yhat], color='darkred',\n",
    "            linewidth=4, alpha=0.7, label='regression model')\n",
    "    ax.plot([min_x, max_x], [ymean, ymean], color='darkgoldenrod',\n",
    "            linewidth=4, alpha=0.7, label='baseline model')\n",
    "    \n",
    "    ax.set_xlabel('weight', fontsize=16)\n",
    "    ax.set_ylabel('height', fontsize=16)\n",
    "    \n",
    "    plt.title('Regression With Baseline')\n",
    "    plt.legend(loc='upper left')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta0 = mod.intercept_\n",
    "beta1 = mod.coef_[0]\n",
    "plot_regression_wbaseline(weight, height, wr_height_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) the true values versus the predicted values\n",
    "\n",
    "Add a line that would pass through the origin with slope one on the  true values versus the predicted values chart.\n",
    "\n",
    "What do the lines represent in each chart?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_y_yhat(y, yhat):\n",
    "\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    ax = fig.gca()\n",
    "\n",
    "    ax.scatter(y, yhat, color='darkgoldenrod', s=70, label='yhat - true y')\n",
    "\n",
    "    max_val = np.max(y)\n",
    "    min_val = np.min(y)\n",
    "\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], color='darkgreen',\n",
    "            linewidth=4.0, alpha=0.7, label='perfect model')\n",
    "\n",
    "    ax.set_xlabel('true y', fontsize=16)\n",
    "    ax.set_ylabel('yhat', fontsize=16)\n",
    "\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_y_yhat(height, wr_height_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## $R^2$ of the regression\n",
    "\n",
    "Recall that the $R^2$ metric calculates the variance explained by your model over the baseline model.\n",
    "\n",
    "The formula, to refresh your memory, is:\n",
    "\n",
    "### $$ R^2 = 1 - \\frac{var(residuals)}{var(y)} $$\n",
    "\n",
    "### 4. Calculate the $R^2$ either by hand or using sklearn or statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wr_height_r2 = r2_score(...?)\n",
    "\n",
    "print ('regression R^2:', wr_height_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5. Remove outliers and build a non-outlier regression (15min)\n",
    "\n",
    "Set a criteria for outliers that removes any value deviating more than 1.5 standard deviations from the mean. (Extremely strict).\n",
    "\n",
    "Build a new regression with the non-outlier values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame({...\n",
    "# mask = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New regression\n",
    "# mod_no = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 6. Plot the regression with the outliers and without the outliers\n",
    "\n",
    "Use the full data for both (not the data with outliers removed).\n",
    "\n",
    "How do the regression lines change (if at all). Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr_height_hat_no = mod_no.predict(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regressions(x, y, yhat1, yhat2):\n",
    "    \n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    ax = fig.gca()\n",
    "        \n",
    "    ax.scatter(x, y, color='steelblue', s=70, label='true y', alpha=0.5)\n",
    "    \n",
    "    min_x, max_x = np.min(x), np.max(x)\n",
    "    \n",
    "    min_yhat1 = np.min(yhat1)\n",
    "    max_yhat1 = np.max(yhat1)\n",
    "    \n",
    "    min_yhat2 = np.min(yhat2)\n",
    "    max_yhat2 = np.max(yhat2)\n",
    "    \n",
    "    ax.plot([min_x, max_x], [min_yhat1, max_yhat1], color='darkred',\n",
    "            linewidth=4, alpha=0.7, label='first regression model')\n",
    "    ax.plot([min_x, max_x], [min_yhat2, max_yhat2], color='darkgoldenrod',\n",
    "            linewidth=4, alpha=0.7, label='no outlier regression model')\n",
    "    \n",
    "    ax.set_xlabel('weight', fontsize=16)\n",
    "    ax.set_ylabel('height', fontsize=16)\n",
    "    \n",
    "    plt.legend(loc='upper left')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regressions(weight, height, wr_height_hat, wr_height_hat_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 7. Calculate the $R^2$ of your outlier-removed model and compare it to the original model\n",
    "\n",
    "Which performs better? Why do you think that is?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr_height_no_r2 = r2_score(height_no, height_no_hat)\n",
    "\n",
    "print ('regression R^2:', wr_height_r2)\n",
    "print ('regression outliers removed R^2:', wr_height_no_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Examining residuals\n",
    "\n",
    "Looking at the residuals (errors) of your model is a good practice. Normally distributed residuals indicate that the assumptions of linear regression are probably being met, which in turn means that your regression is modeling the linear relationship appropriately.\n",
    "\n",
    "### 8. Plot a histogram of the residuals from the original and no-outlier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_resid = height - wr_height_hat\n",
    "height_no_resid = height - wr_height_hat_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(height_resid, bins=40, hist=True, kde=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(height_no_resid, bins=40, hist=True, kde=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Validating the model against a test set\n",
    "\n",
    "Load in the test set file for the combine data. Pull out the corresponding target and predictor variables for the test set.\n",
    "\n",
    "It is best practice after you build a model to, if possible, validate it against held out data. If it performs as well or nearly as well, you can be more sure that the model you've created is in fact making a correct inference about the linear relationship between variables for the overall population.\n",
    "\n",
    "### 9. Get the $R^2$ value for your original model predicting values from the test data\n",
    "\n",
    "Compare this to the $R^2$ on your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_test = pd.read_csv('./datasets/combine_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrs_test = combine_test[combine_test.Position == 'WR']\n",
    "weight_test = wrs_test[['Weight']].values\n",
    "height_test = wrs_test.HeightInchesTotal.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_test_yhat = \n",
    "height_test_r2 = \n",
    "\n",
    "print ('R^2 on train set:', wr_height_r2)\n",
    "print ('R^2 on test set:', height_test_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Calculate the mse for the test data and baseline model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_test_mse = mean_squared_error(height_test, height_test_yhat)\n",
    "base_test_mse = mean_squared_error(height_test, np.tile(np.mean(height_test), len(height_test)))\n",
    "\n",
    "print ('Model on test mse:', mod_test_mse)\n",
    "print ('Baseline test mse:', base_test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Plot the regression applied to test data against the test data baseline model\n",
    "\n",
    "Look visually how it performs versus just guessing the mean of the target in the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tips: use the function plot_regression_wbaseline we defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "* Main types of loss functions\n",
    "* Ordinary least squares regression with statsmodels and sklearn\n",
    "* model evaluation with mean squared error and R2\n",
    "* examination of residuals\n",
    "* tested the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
