{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Web Scraping\n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Understand motivation for web scraping:\n",
    "    * What does a web data pipeline look like?\n",
    "    * How should we store data from the web?\n",
    "2. Know high level differences between NoSQL and SQL.\n",
    "\n",
    "\n",
    "<div style=\"text-align: center\"><h3>The Reality of Scraping</h3><img src=\"images/scraping_meme.png\" style=\"width: 600px\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we scrape the web?\n",
    "\n",
    "* Realistically, data that you want to study won't always be available to you in the form of a curated data set.\n",
    "* Need to go to the internets to find interesting data:\n",
    "    * From an existing company\n",
    "    * Text for NLP\n",
    "    * Images\n",
    "    <div style=\"text-align: center\"><h3>Web Data Pipeline</h3><img src=\"images/web_data_pipeline.png\" style=\"width: 600px\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing data from the web\n",
    "\n",
    "* We have seen how to store data -> SQL (RBDMS).\n",
    "    * Why wouldn't SQL necessarily be the best tool for storing data that we retrieve from the web?\n",
    "        * Data are messy!\n",
    "* Enter No SQL. Stands for **N**ot **o**nly **SQL**. MongoDB is a flavor of NoSQL, like PosgreSQL is a flavor of SQL.\n",
    "    * A NoSQL paradigm may be preferable to SQL because it is **schemaless**.\n",
    "    * Great for **storing unstructured data**, as we may find on the web!\n",
    "    * MongoDB is a document-oriented DBMS:\n",
    "      <div style=\"text-align: center\"><h3>Centered around \"Documents\"</h3><img src=\"images/document_based_storage.png\" style=\"width: 600px\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL vs. Mongo\n",
    "\n",
    "* SQL - want to prevent redundancy in data by having **tables with unique information and relations** between them (normalized data).\n",
    "    * Creates a **framework for querying** with joins.\n",
    "    * Makes it easier to update database. Only ever have to **change information in a single place**.\n",
    "    * This can result in **\"simple\" queries being slower, but more complex queries are often faster**.\n",
    "* Mongo - **document based storage system**. Does not enforce normalized data. Can have data **redundancies in documents** (denormalized data).\n",
    "    * **No joins**.\n",
    "    * A change to database generally results in needing to **change many documents**.\n",
    "    * Since there is redundancy in the documents, **simple queries are generally faster. But complex queries are often slower**.\n",
    "    \n",
    "\n",
    "|         | SQL          | Mongo          |\n",
    "|---------|--------------|----------------|\n",
    "| Schema  | Yes => Joins | No => No Joins |\n",
    "| Storage | Table        | Collection     |\n",
    "|         | Row          | Document       |\n",
    "|         | Column       | Field          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping from a Web Page with Python\n",
    "\n",
    "Scraping a web site basically comes down to making a **request from Python and parsing through the HTML** that is returned from each page. For each of these tasks we have a Python library, **`requests` and `bs4`**, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Info from a Web Page\n",
    "\n",
    "Now that we can gain easy access to the HMTL for a web page, we need **some way to pull the desired content from it**. Luckily there is already a system in place to do this. With a **combination of HMTL and CSS selectors** we can identify the information on a HMTL page that we wish to retrieve and grab it with [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#searching-the-tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<title>The title of this web page</title>\n",
    "</head>\n",
    "<body>\n",
    "<h1>My Photos</h1>\n",
    "<div class='intro'>\n",
    "<p>These are some photos of my trips.</p>\n",
    "<img src=\"me.png\">\n",
    "</div>\n",
    "\n",
    "<h3>Italy</h3>\n",
    "<div class='country'>\n",
    "<img src=\"venice1.png\" alt=\"Venice\"> <br />\n",
    "<img src=\"venice2.png\" alt=\"Venice\"> <br />\n",
    "<img src=\"rome.png\" alt=\"Roma\">\n",
    "</div>\n",
    "\n",
    "<h3>Germany</h3>\n",
    "<div class='country'>\n",
    "<img src=\"berlin.png\" alt=\"Berlin\">\n",
    "</div>\n",
    "</body>\n",
    "</html>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# we create a soup object with the html:\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>The title of this web page</title>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can query it\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The title of this web page'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1>My Photos</h1>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h3>Italy</h3>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h3>Italy</h3>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('h3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h3>Italy</h3>, <h3>Germany</h3>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('h3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Germany'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('h3')[1].string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"country\">\n",
       " <img alt=\"Venice\" src=\"venice1.png\"/> <br/>\n",
       " <img alt=\"Venice\" src=\"venice2.png\"/> <br/>\n",
       " <img alt=\"Roma\" src=\"rome.png\"/>\n",
       " </div>,\n",
       " <div class=\"country\">\n",
       " <img alt=\"Berlin\" src=\"berlin.png\"/>\n",
       " </div>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('div', class_='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<img alt=\"Venice\" src=\"venice1.png\"/>, <img alt=\"Venice\" src=\"venice2.png\"/>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('img', alt='Venice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h3>Italy</h3>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('div', class_='country').find_previous_siblings('h3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:white\">\n",
    "for div in soup.find_all('div', class_='country'):\n",
    "    h3 = div.find_previous_siblings('h3')[0]\n",
    "    country = h3.string\n",
    "    print(country)\n",
    "\n",
    "for div in soup.find_all('div', class_='country'):\n",
    "    h3 = div.find_previous_siblings('h3')[0]\n",
    "    country = h3.string\n",
    "    for img in div.find_all('img'):\n",
    "        image = img.get('src')\n",
    "        print('Country: {}: image: {}'.format(country, image))\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Info from a Web Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requests Library\n",
    "\n",
    "The [requests](http://docs.python-requests.org/en/latest/index.html) library is designed to simplify the process of making **http requests within Python**. The interface is mind-bogglingly simple. Instantiate a requests object to the request, this will mostly be a `get`, with the URL and optional parameters you'd like passed through the request. That instance make the results of the request available via attributes/methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "fun_cheap = 'http://sf.funcheap.com'\n",
    "r = requests.get('http://sf.funcheap.com/2018/06/25/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\\n<html xmlns=\"https://www.w3.org/1999/xhtml\" lang=\"en-US\" xmlns:fb=\"https://www.facebook.com/2008/fbml\" xmlns:addthis=\"https://www.addthis.com/help/api-spec\" >\\n\\n<head profile=\"https://gmpg.org/xfn/11\">\\n\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" />\\n\\n\\n<title>Events for June 25, 2018 - Funcheap</title>\\n\\n<meta name=\"generator\" content=\"WordPress\" /> <!-- leave this for stats -->\\n\\n<link rel=\"stylesheet\" href=\"https://cdn.funcheap.com/wp-content/themes/arthemia-premium/style.css?v=1.8.23\" type=\"text/css\" media=\"screen\" />\\n<link rel=\"stylesheet\" href=\"https://cdn.funcheap.com/wp-content/themes/arthemia-premium/madmenu.css?v=1.1\" type=\"text/css\" media=\"screen\" />\\n<!--[if IE 6]>\\n    <style type=\"text/css\">\\n    body {\\n        behavior:url(\"https://cdn.funcheap.com/wp-content/themes/arthemia-premium/scripts/csshover2.htc\");\\n    }\\n    </style>\\n<![endif]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.text[:1000] # First 1000 characters of the HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have the web page, we can parse it with beautifulsoup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the title of the page using the tag 'title':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Events for  June 25, 2018'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('h2.title')[0].string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h2 class=\"title\">Events for  June 25, 2018</h2>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = soup.find_all('h2', class_='title')[0]\n",
    "\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_clear_float = title.next_sibling.next_sibling\n",
    "\n",
    "# good_clear_float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same all the urls under the 'a' tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "for tag in good_clear_float.find_all('a', rel=True):\n",
    "    href = tag.attrs['href']\n",
    "    urls.append(href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
